{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import np_utils\n",
    "from PIL import Image\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import array_to_img\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#作成するモデルの保存用\n",
    "#フェーズ視認用\n",
    "result_folder=\"2DCNN_results\"\n",
    "date_folder=20221226\n",
    "title=\"2DCNN_Ct_machinelearning\" \n",
    "models=\"ResNet50\"\n",
    "case=2  #分類器の選択\n",
    "#optimizer の選択\n",
    "opt=\"Adam\"\n",
    "#opt='AMSGrad'\n",
    "#opt=\"RMS\"\n",
    "#opt='SGD'\n",
    "#opt='adamax'\n",
    "\n",
    "dataset=\"allequalver\"\n",
    "\n",
    "#学習に関するハイパパラメータ\n",
    "epochs=300\n",
    "\n",
    "#画像サイズ\n",
    "size=224\n",
    "num_class=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSVファイル読み込みの関数\n",
    "# ラベル付け\n",
    "def label_value(serch):\n",
    "    if \"acceptable\" in serch:\n",
    "        value=0\n",
    "    else :\n",
    "        value=1\n",
    "        \n",
    "    return value\n",
    "\n",
    "#データの読み込みのために、csv読み込み\n",
    "def read_path_and_tag(csv_f):\n",
    "    train=[]\n",
    "    train_label=[]\n",
    "    train_tag=[]\n",
    "    valid=[]\n",
    "    valid_label=[]\n",
    "    valid_tag=[]\n",
    "    test=[]\n",
    "    test_label=[]\n",
    "    test_tag=[]\n",
    "\n",
    "    with open(csv_f) as f:\n",
    "        reader =csv.reader(f)\n",
    "        for row in reader:\n",
    "\n",
    "            if row[1] == \"test\":\n",
    "                test.append(row[0])\n",
    "                test_label.append(label_value(row[0]))\n",
    "                test_tag.append(row[1])\n",
    "\n",
    "            elif row[1]==\"valid\":\n",
    "                valid.append(row[0])\n",
    "                valid_label.append(label_value(row[0]))\n",
    "                valid_tag.append(row[1])\n",
    "\n",
    "            elif row[1]==\"train\":\n",
    "                train.append(row[0])\n",
    "                train_label.append(label_value(row[0]))\n",
    "                train_tag.append(row[1])\n",
    "\n",
    "    return train,train_label,train_tag,valid,valid_label,valid_tag,test,test_label,test_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSVファイルからパスの取得\n",
    "CTace_path =  read_path_and_tag(\"file_way_CT_1of2_2DForUse_retable.csv\")\n",
    "CTcol_path =  read_path_and_tag(\"file_way_CT_2of2_2DForUse_retable.csv\")\n",
    "\n",
    "array_name = [\"train\",\"train_label\",\"train_tag\",\"valid\",\"valid_label\",\"valid_tag\",\"test\",\"test_label\",\"test_tag\"]\n",
    "array = [\"train\",\"train_label\",\"train_tag\",\"valid\",\"valid_label\",\"valid_tag\",\"test\",\"test_label\",\"test_tag\"]\n",
    "\n",
    "for i in range(9):\n",
    "    array[i] = CTace_path[i]+CTcol_path[i]\n",
    "    \n",
    "train = array[0]\n",
    "train_label = array[1]\n",
    "train_tag = array[2]\n",
    "valid = array[3]\n",
    "valid_label = array[4]\n",
    "valid_tag = array[5]\n",
    "test = array[6]\n",
    "test_label = array[7]\n",
    "test_tag = array[8]\n",
    "\n",
    "for i in range(9):\n",
    "    print(array_name[i],len(array[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add validation data to training data for cross_validation\n",
    "trainvalid = []\n",
    "trainvalid_label = []\n",
    "trainvalid_tag = []\n",
    "\n",
    "trainvalid = train + valid\n",
    "trainvalid_label = train_label + valid_label\n",
    "trainvalid_tag = train_tag + valid_tag\n",
    "# print(len(trainvalid),trainvalid)\n",
    "# print(len(trainvalid_label),trainvalid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 少ないほうを多いほうと同数にコピー\n",
    "trainvalid_0=[]\n",
    "trainvalid_label_0=[]\n",
    "trainvalid_tag_0=[]\n",
    "trainvalid_1=[]\n",
    "trainvalid_label_1=[]\n",
    "trainvalid_tag_1=[]\n",
    "\n",
    "cnt=0\n",
    "for i in trainvalid_label:\n",
    "    if i == 0:\n",
    "        tmp=trainvalid[cnt]\n",
    "        trainvalid_0.append(tmp)\n",
    "        labeltmp=trainvalid_label[cnt]\n",
    "        trainvalid_label_0.append(labeltmp)\n",
    "        tagtmp=trainvalid_tag[cnt]\n",
    "        trainvalid_tag_0.append(tagtmp)\n",
    "    elif i == 1:\n",
    "        tmp=trainvalid[cnt]\n",
    "        trainvalid_1.append(tmp)\n",
    "        labeltmp=trainvalid_label[cnt]\n",
    "        trainvalid_label_1.append(labeltmp)\n",
    "        tagtmp=trainvalid_tag[cnt]\n",
    "        trainvalid_tag_1.append(tagtmp)\n",
    "    cnt+=1\n",
    "    \n",
    "trainvalid_imfalted=[]\n",
    "trainvalid_label_imfalted=[]\n",
    "trainvalid_tag_imfalted=[]\n",
    "if len(trainvalid_0)<len(trainvalid_1):\n",
    "    trainvalid_add=len(trainvalid_1)-len(trainvalid_0)\n",
    "    for i in range(100):\n",
    "        for n in range(len(trainvalid_0)):\n",
    "            if len(trainvalid_imfalted) >= trainvalid_add:\n",
    "                break        \n",
    "            trainvalid_imfalted.append(trainvalid_0[n])\n",
    "            trainvalid_label_imfalted.append(trainvalid_label_0[n])\n",
    "            trainvalid_tag_imfalted.append(trainvalid_tag_0[n])\n",
    "\n",
    "    #もとのデータと結合＝＞同数の調整完了\n",
    "    trainvalid = trainvalid + trainvalid_imfalted  # リスト型同士の足し合わせ\n",
    "\n",
    "    trainvalid_label = trainvalid_label + trainvalid_label_imfalted  # 初期のリスト型\n",
    "\n",
    "    trainvalid_tag = trainvalid_tag + trainvalid_tag_imfalted\n",
    "\n",
    "    print(len(trainvalid),len(trainvalid_label),len(trainvalid_tag))\n",
    "\n",
    "elif len(trainvalid_1)<len(trainvalid_0):\n",
    "    trainvalid_add=len(trainvalid_0)-len(trainvalid_1)\n",
    "    for i in range(100):\n",
    "        for n in range(len(trainvalid_1)):\n",
    "            if len(trainvalid_imfalted) >= trainvalid_add:\n",
    "                break        \n",
    "            trainvalid_imfalted.append(trainvalid_1[n])\n",
    "            trainvalid_label_imfalted.append(trainvalid_label_1[n])\n",
    "            trainvalid_tag_imfalted.append(trainvalid_tag_1[n])\n",
    "            \n",
    "    #もとのデータと結合＝＞同数の調整完了\n",
    "    trainvalid = trainvalid + trainvalid_imfalted  # リスト型同士の足し合わせ\n",
    "\n",
    "    trainvalid_label = trainvalid_label + trainvalid_label_imfalted  # 初期のリスト型\n",
    "\n",
    "    trainvalid_tag = trainvalid_tag + trainvalid_tag_imfalted\n",
    "\n",
    "    print(len(trainvalid),len(trainvalid_label),len(trainvalid_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# チェック\n",
    "trainvalid_0=[]\n",
    "trainvalid_1=[]\n",
    "trainvalid_label_0=[]\n",
    "trainvalid_label_1=[]\n",
    "\n",
    "\n",
    "for i in range(len(trainvalid_label)):\n",
    "    if trainvalid_label[i]==0:\n",
    "        trainvalid_0.append(trainvalid[i])\n",
    "        trainvalid_label_0.append(0)\n",
    "    elif trainvalid_label[i]==1:\n",
    "        trainvalid_1.append(trainvalid[i])\n",
    "        trainvalid_label_1.append(1)\n",
    "print(len(trainvalid_0),len(trainvalid_1),len(trainvalid_label_0),len(trainvalid_label_1))\n",
    "#print(trainvalid_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainデータの読み込み\n",
    "from matplotlib import pyplot as plt\n",
    "trainvalid_0_img = []\n",
    "trainvalid_1_img = []\n",
    "for img in trainvalid_0:\n",
    "    trainvalid_0_img.append(cv2.imread(img))\n",
    "\n",
    "for img in trainvalid_1:\n",
    "    trainvalid_1_img.append(cv2.imread(img))\n",
    "    \n",
    "print(len(trainvalid_0),len(trainvalid_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  パスのダブりを解消、区別をつける\n",
    "for i in range(len(trainvalid_0)):\n",
    "    cnt = 1\n",
    "    for t in range(len(trainvalid_0)):\n",
    "        if i!=t:\n",
    "            if trainvalid_0[i]==trainvalid_0[t]:\n",
    "                if \".png\" in trainvalid_0[t]:\n",
    "                    if cnt==1:\n",
    "                        trainvalid_0[t]=trainvalid_0[t].replace('.png','_copy'+str(cnt)+'.png')\n",
    "                        cnt += 1\n",
    "                        \n",
    "                    elif cnt==2:\n",
    "                        trainvalid_0[t]=trainvalid_0[t].replace('.png','_copy'+str(cnt)+'.png')\n",
    "                        cnt += 1\n",
    "                    \n",
    "                    elif cnt==3:\n",
    "                        trainvalid_0[t]=trainvalid_0[t].replace('.png','_copy'+str(cnt)+'.png')\n",
    "\n",
    "# 解消できたかチェック\n",
    "for i in range(len(trainvalid_0)):\n",
    "    for t in range(len(trainvalid_0)):\n",
    "        if i!=t:\n",
    "            if trainvalid_0[i]==trainvalid_0[t]:\n",
    "                print(t,trainvalid_0[t])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  パスのダブりを解消、区別をつける\n",
    "for i in range(len(trainvalid_1)):\n",
    "    cnt = 1\n",
    "    for t in range(len(trainvalid_1)):\n",
    "        if i!=t:\n",
    "            if trainvalid_1[i]==trainvalid_1[t]:\n",
    "                if \".png\" in trainvalid_1[t]:\n",
    "                    if cnt==1:\n",
    "                        trainvalid_1[t]=trainvalid_1[t].replace('.png','_copy'+str(cnt)+'.png')\n",
    "                        cnt += 1\n",
    "                        \n",
    "                    elif cnt==2:\n",
    "                        trainvalid_1[t]=trainvalid_1[t].replace('.png','_copy'+str(cnt)+'.png')\n",
    "                        cnt += 1\n",
    "                    \n",
    "                    elif cnt==3:\n",
    "                        trainvalid_1[t]=trainvalid_1[t].replace('.png','_copy'+str(cnt)+'.png')\n",
    "\n",
    "# 解消できたかチェック\n",
    "for i in range(len(trainvalid_1)):\n",
    "    for t in range(len(trainvalid_1)):\n",
    "        if i!=t:\n",
    "            if trainvalid_1[i]==trainvalid_1[t]:\n",
    "                print(t,trainvalid_1[t])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testも同数に\n",
    "test_0=[]\n",
    "test_1=[]\n",
    "pickup_test=[]\n",
    "test_equal=[]\n",
    "random.seed(1)\n",
    "\n",
    "for i in test:\n",
    "    if \"acceptable\" in i:\n",
    "        test_0.append(i)\n",
    "    else:\n",
    "        test_1.append(i)\n",
    "\n",
    "if len(test_0)<len(test_1):\n",
    "    pickup_test=random.sample(test_1,len(test_0))\n",
    "    test_equal=test_0+pickup_test      \n",
    "elif len(test_1)<len(test_0):\n",
    "    pickup_test=random.sample(test_0,len(test_1))\n",
    "    test_equal=test_1+pickup_test \n",
    "\n",
    "# test_equalのラベル作成\n",
    "test_equal_label=[]\n",
    "for i in test_equal:\n",
    "    if \"acceptable\" in i:\n",
    "        test_equal_label.append(0)\n",
    "            \n",
    "    else:\n",
    "        test_equal_label.append(1)\n",
    "            \n",
    "test=[]\n",
    "test_label=[]\n",
    "for i in range(len(test_equal_label)):\n",
    "    test.append(test_equal[i])\n",
    "    test_label.append(valid_equal_label[i])\n",
    "print(len(test),len(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testデータの読み込み\n",
    "test_img = []\n",
    "for img in test_equal:\n",
    "    test_img.append(cv2.imread(img))\n",
    "    \n",
    "print(len(test_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用する画像をresizeする\n",
    "for i in range(len(trainvalid_1_img)):\n",
    "    trainvalid_1_img[i] = cv2.resize(trainvalid_1_img[i],(224,224))\n",
    "    trainvalid_0_img[i] = cv2.resize(trainvalid_0_img[i],(224,224))\n",
    "    \n",
    "for i in range(len(test_img)):\n",
    "    test_img[i] = cv2.resize(test_img[i],(224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(trainvalid_0_img))\n",
    "print(trainvalid_0_img[0].shape)\n",
    "print(type(test_img[0]))\n",
    "print(test_img[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#画像の表示　内容の確認\n",
    "from matplotlib import pyplot as plt\n",
    "for i in range(4):\n",
    "    plt.subplot(2,4,i+1)\n",
    "    plt.imshow(trainvalid_0_img[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np配列に変換\n",
    "trainvalid_0_img2 = np.asarray(trainvalid_0_img)\n",
    "trainvalid_1_img2 = np.asarray(trainvalid_1_img)\n",
    "test_img = np.asarray(test_img)\n",
    "trainvalid_label_0_ndarray = np.asarray(trainvalid_label_0)\n",
    "trainvalid_label_1_ndarray = np.asarray(trainvalid_label_1)\n",
    "print(type(trainvalid_0_img2))\n",
    "print(type(trainvalid_label_0_ndarray))\n",
    "print(trainvalid_label_0_ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存に使うパスの作成用\n",
    "def folder_path(result_folder,date_folder,title,model,case,opt,dataset,train_amount,cnt,rate,batch,epochs):#  保存に使う\n",
    "    result = result_folder + '/'\n",
    "    date = str(date_folder) +'_results/'\n",
    "    path = title + \"-NN\" + str(case) + \"/\" + opt + '/'\n",
    "    select =  dataset + '/'+\"train_\" + str(train_amount) + '_rate_' + str(rate) + '_batch_' + str(batch) + '_epochs_' + str(epochs) + '_optimizer_' + opt +\"/crosval_cnt\"+str(cnt)+\"/\"\n",
    "    \n",
    "    save_name = result + date + path + select\n",
    "    save_view_name = \"NN\" + str(case) + \"_\" + opt + '_' + select\n",
    "    \n",
    "    return save_name,save_view_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの構築\n",
    "#モデルの構築\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.models import Model,Sequential\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Conv2D,MaxPooling2D,Input,Dense,Dropout,Flatten,Activation,GlobalAveragePooling2D\n",
    "from keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras import optimizers\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "def make_model(size,numclass,optimizer,case,mode,read_name):\n",
    "    #ResNet50のロード　（FC層を除く\n",
    "    input_tensor=Input(shape=(size,size,3))\n",
    "    resnet50=ResNet50(include_top=False,\n",
    "                      weights='imagenet',\n",
    "                      input_tensor=input_tensor)\n",
    "    \n",
    "     #最下層の出力層のオリジナルモデル部分をsequentialモデルで作成\n",
    "    top_model=Sequential()\n",
    "    top_model.add(Flatten(input_shape=resnet50.output_shape[1:]))\n",
    "    if case==1:\n",
    "        top_model.add(Dense(numclass,activation='softmax'))\n",
    "    elif case==2:\n",
    "        top_model.add(Dense(256,activation='relu'))\n",
    "        top_model.add(Dropout(0.1))\n",
    "        top_model.add(Dense(128,activation='relu'))\n",
    "        top_model.add(Dropout(0.1))\n",
    "        top_model.add(Dense(64,activation='relu'))\n",
    "        top_model.add(Dropout(0.5))\n",
    "        top_model.add(Dense(numclass,activation='softmax'))\n",
    "    elif case==3:\n",
    "        top_model.add(Dense(256,activation='relu'))\n",
    "        top_model.add(Dropout(0.5))\n",
    "        top_model.add(Dense(128,activation='relu'))\n",
    "        top_model.add(Dropout(0.5))\n",
    "        top_model.add(Dense(64,activation='relu'))\n",
    "        top_model.add(Dropout(0.5))\n",
    "        top_model.add(Dense(numclass,activation='softmax'))\n",
    "    elif case==4:\n",
    "        top_model.add(Dense(256,activation='relu'))\n",
    "        top_model.add(Dropout(0.3))\n",
    "        top_model.add(Dense(128,activation='relu'))\n",
    "        top_model.add(Dropout(0.3))\n",
    "        top_model.add(Dense(64,activation='relu'))\n",
    "        top_model.add(Dropout(0.3))\n",
    "        top_model.add(Dense(32,activation='relu'))\n",
    "        top_model.add(Dropout(0.3))\n",
    "        top_model.add(Dense(16,activation='relu'))\n",
    "        top_model.add(Dropout(0.3))\n",
    "        top_model.add(Dense(numclass,activation='softmax'))\n",
    "    elif case==5:\n",
    "        top_model.add(Dense(256,activation='relu'))\n",
    "        top_model.add(Dropout(0.5))\n",
    "        top_model.add(Dense(128,activation='relu'))\n",
    "        top_model.add(BatchNormalization())\n",
    "        top_model.add(Dense(64,activation='relu'))\n",
    "        top_model.add(Dropout(0.5))\n",
    "        top_model.add(Dense(numclass,activation='softmax'))\n",
    "    elif case==6:\n",
    "        top_model.add(Dense(256,activation='relu'))\n",
    "        top_model.add(BatchNormalization())\n",
    "        top_model.add(Dense(numclass,activation='softmax'))\n",
    "    \n",
    "    #ResNet50とオリジナル部分を結合してモデルの作成\n",
    "    model=Model(inputs=resnet50.input,\n",
    "                outputs=top_model(resnet50.output))\n",
    "    \n",
    "   # resnet50.summary()\n",
    "    model.summary()\n",
    "    \n",
    "\n",
    "    #resnet50部分の重みを固定（転移学習）\n",
    "    for layer in resnet50.layers:\n",
    "        layer.trainable=False\n",
    "    \n",
    "    #多クラス分類を指定\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                    optimizer=optimizer,\n",
    "                    metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "    \n",
    "\n",
    "import pickle\n",
    "def save_model_data(save_name,model,hist,test_img):\n",
    "    #モデル構造の保存\n",
    "    open(save_name +\"model.json\",\"w\").write(model.to_json())\n",
    "\n",
    "    #学習済みの重みを保存\n",
    "    model.save_weights(save_name+\"weight.hdf5\")\n",
    "\n",
    "    #学習履歴を保存\n",
    "    with open(save_name+\"result.jdon\",\"wb\") as f:\n",
    "        pickle.dump(hist.history,f)\n",
    "        \n",
    "    #  予測確率を保存\n",
    "    with open(save_name + \"preds.jdon\",\"wb\") as f:\n",
    "        pickle.dump(model.predict(test_img),f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#評価の関数\n",
    "#学習歴を保存\n",
    "def plot_history(history,save_path,fig_size_width,fig_size_height,lim_font_size):\n",
    "    #  まずは訓練\n",
    "    acc=history.history['accuracy']\n",
    "    loss =history.history['loss']\n",
    "   \n",
    "    epochs=range(len(acc))\n",
    "    \n",
    "    plt.figure(figsize=(fig_size_width,fig_size_height))\n",
    "    plt.rcParams['font.family']='Times New Roman'\n",
    "    plt.rcParams['font.size']=lim_font_size\n",
    "    \n",
    "    plt.plot(epochs,acc,color=\"blue\",linestyle=\"solid\",label='train acc') \n",
    "    plt.plot(epochs,loss,color=\"red\",linestyle=\"dashed\",label='train loss')\n",
    "    plt.ylim(bottom=0)\n",
    "   \n",
    "    train_graph=save_path+'hist_train_record.png'\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.savefig(train_graph)\n",
    "    plt.close()\n",
    "    \n",
    "#     次は検証\n",
    "    plt.figure(figsize=(fig_size_width,fig_size_height))\n",
    "    plt.rcParams['font.family']='Times New Roman'\n",
    "    plt.rcParams['font.size']=lim_font_size\n",
    "    \n",
    "    val_acc=history.history['val_accuracy']\n",
    "    val_loss=history.history['val_loss']\n",
    "    \n",
    "    plt.plot(epochs,val_acc,color=\"green\",linestyle=\"dashdot\",label='valid acc')\n",
    "    plt.plot(epochs,val_loss,color=\"orange\",linestyle=\"dotted\",label='valid loss')\n",
    "    plt.ylim(bottom=0)\n",
    "    \n",
    "    valid_graph=save_path+'hist_valid_record.png'\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.savefig(valid_graph)\n",
    "    plt.close()\n",
    "    \n",
    "#     最後は混合\n",
    "    plt.figure(figsize=(fig_size_width,fig_size_height))\n",
    "    plt.rcParams['font.family']='Times New Roman'\n",
    "    plt.rcParams['font.size']=lim_font_size\n",
    "\n",
    "    plt.plot(epochs,acc,color=\"blue\",linestyle=\"solid\",label='train acc')\n",
    "    plt.plot(epochs,val_acc,color=\"green\",linestyle=\"dashdot\",label='valid acc')\n",
    "    plt.plot(epochs,loss,color=\"red\",linestyle=\"dashed\",label='train loss')\n",
    "    plt.plot(epochs,val_loss,color=\"orange\",linestyle=\"dotted\",label='valid loss')\n",
    "    plt.ylim(bottom=0)\n",
    "    \n",
    "    mix_graph = save_path+'hist.png'\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.savefig(mix_graph)\n",
    "    plt.close()\n",
    "    \n",
    "#     accのみ\n",
    "    plt.figure(figsize=(fig_size_width,fig_size_height))\n",
    "    plt.rcParams['font.family']='Times New Roman'\n",
    "    plt.rcParams['font.size']=lim_font_size\n",
    "\n",
    "    plt.plot(epochs,acc,color=\"blue\",linestyle=\"solid\",label='train acc')\n",
    "    plt.plot(epochs,val_acc,color=\"green\",linestyle=\"dashdot\",label='valid acc')\n",
    "    plt.ylim(bottom=0)\n",
    "    \n",
    "    acc_graph = save_path+'acc.png'\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.savefig(acc_graph)\n",
    "    plt.close()\n",
    "    \n",
    "#     lossのみ\n",
    "    plt.figure(figsize=(fig_size_width,fig_size_height))\n",
    "    plt.rcParams['font.family']='Times New Roman'\n",
    "    plt.rcParams['font.size']=lim_font_size\n",
    "    \n",
    "    plt.plot(epochs,loss,color=\"red\",linestyle=\"dashed\",label='train loss')\n",
    "    plt.plot(epochs,val_loss,color=\"orange\",linestyle=\"dotted\",label='valid loss')\n",
    "    plt.ylim(bottom=0)\n",
    "    \n",
    "    loss_graph = save_path+'loss.png'\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.savefig(loss_graph)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "##コンフュージョンマトリックス\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "def Confusion_matrix(model,target,label,cm_name,color,view_name):\n",
    "    #  予測の出力　　リストの０番目の要素の方が大きいときは陰性予想、１番目の方が大きいときは陽性予想\n",
    "    preds=model.predict(target)\n",
    "    \n",
    "    #  ０と１番目の要素を比べ値が大きいほうの要素番号をリストとして保存\n",
    "    #  ラベルと照らし合わせるためにpredsから０と１の予想リストを作成\n",
    "    pred_list=[]\n",
    "    for pred in preds:\n",
    "        pred_list.append(np.argmax(pred))\n",
    "    \n",
    "    tn,fp,fn,tp=confusion_matrix(label,pred_list).ravel()\n",
    "    acc=metrics.accuracy_score(label,pred_list)  #  accuracy\n",
    "    ppv=metrics.precision_score(label,pred_list) #  precision(精度、適合率)\n",
    "    tpr=metrics.recall_score(label,pred_list)    #　recall ,true positive rate(再現率、真陽性率)\n",
    "    tnr=tn/(tn+fp)                               #  true negative rate(真陰性率、特異度)\n",
    "    fnr=fn/(tp+fn)                               #  false negative rate(偽陰性率)\n",
    "    fpr=fp/(fp+tn)                               #  false positive rate(偽陽性率)\n",
    "    \n",
    "    #  数値の書式を設定\n",
    "    acc='{:.4f}'.format(acc)\n",
    "    ppv='{:.4f}'.format(ppv)\n",
    "    tpr='{:.4f}'.format(tpr)\n",
    "    tnr='{:.4f}'.format(tnr)\n",
    "    fnr='{:.4f}'.format(fnr)\n",
    "    fpr='{:.4f}'.format(fpr)\n",
    "    \n",
    "    \n",
    "    plt.figure()\n",
    "    c_m=confusion_matrix(label,pred_list)  #  混合行列を取得\n",
    "    #  混合行列のレイアウトを設定\n",
    "    plt.text(0,3.0,view_name,size=17)\n",
    "    plt.text(0.4,0.3,\"TN\")\n",
    "    plt.text(1.4,0.3,\"FP\")\n",
    "    plt.text(0.4,1.3,\"FN\")\n",
    "    plt.text(1.4,1.3,\"TP\")\n",
    "    plt.text(2.6,0.2,\"ACC:\"+str(acc))\n",
    "    plt.text(2.6,0.6,\"PPV:\"+str(ppv))\n",
    "    plt.text(2.6,1.0,\"TPR:\"+str(tpr))\n",
    "    plt.text(2.6,1.4,\"TNR:\"+str(tnr))\n",
    "    plt.text(2.6,1.8,\"FNR:\"+str(fnr))\n",
    "    plt.text(2.6,2.2,\"FPR:\"+str(fpr)) \n",
    "    \n",
    "    sns.heatmap(c_m,annot=True,cmap=color,fmt='g')\n",
    "    plt.xlabel(\"predict\")\n",
    "    plt.ylabel(\"actual\")\n",
    "    plt.savefig(cm_name,bbox_inches='tight') #  描画範囲からはみ出ないようにする\n",
    "    plt.close()\n",
    "    return pred_list\n",
    "\n",
    "# ROC曲線の作成\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "\n",
    "def ROC_CURVE(model,img,label,save_name,view_name):\n",
    "#  閾値の列\n",
    "    threshhold = np.arange(0,1,0.01)\n",
    "    n = threshhold.size\n",
    "\n",
    "#  データの作成\n",
    "    a = model.predict(img)  #  0成分：陰性、1成分：陽性  predsが入る\n",
    "    N = len(a)\n",
    "#print(N)\n",
    "    b = np.zeros((n,N))  #  閾値により０と１に振り分けた入れ物\n",
    "    \n",
    "#  陽性側が閾値より大きければ１とする\n",
    "    for t in range(n):\n",
    "        for i in range(N):\n",
    "            if a[i][1]>threshhold[t]:\n",
    "                b[t][i] = 1\n",
    "            \n",
    "#  auc値のために予測と正解ラベルの２重リストを一つのリストにする\n",
    "    b_list=b.tolist()\n",
    "#print(b)\n",
    "            \n",
    "    B=[] # 予測ラベル\n",
    "    C=[] #  正解ラベル\n",
    "    for t in range(n):\n",
    "        for i in range(N):\n",
    "            B.append(b_list[t][i])\n",
    "            C.append(label[i])            \n",
    "# print(C)\n",
    "#print(len(C),len(B))\n",
    "    \n",
    "#  グラフにプロットする準備            \n",
    "    fpr_np=[]\n",
    "    tpr_np=[]\n",
    "    for t in range(n):\n",
    "        fpr_np.append(roc_curve(label,b[t])[0])\n",
    "        tpr_np.append(roc_curve(label,b[t])[1])\n",
    "        tn,fp,fn,tp=confusion_matrix(label,b[t]).ravel()\n",
    "        print('Threshold=',f\"{0+0.01*t:.2f}\",'\\tTPR=',f\"{tp/(tp+fn):.2f}\",'\\tFPR=',f\"{fp/(fp+tn):.2f}\")\n",
    "        fpr=np.array(fpr_np)\n",
    "        tpr=np.array(tpr_np)\n",
    "    print(fpr)\n",
    "    print(tpr)\n",
    "\n",
    "    tprfpr=np.zeros((n,3))\n",
    "    for t in range(n):\n",
    "        tprfpr[t][0]=0+0.01*t\n",
    "        tprfpr[t][1]=tpr[t][1]\n",
    "        tprfpr[t][2]=fpr[t][1]\n",
    "    with open(save_name + \"tprfpr.jdon\",\"wb\") as f:\n",
    "        pickle.dump(tprfpr,f)\n",
    "\n",
    "#  グラフにプロット\n",
    "    plt.figure()\n",
    "    plt.xlabel('FPR:False positive rate')\n",
    "    plt.ylabel('TPR:True positive rate')\n",
    "    plt.grid()\n",
    "    roc=roc_auc_score(C,B)\n",
    "    plt.text(0.5,0.0,\" auc :\"+str('{:.4f}'.format(roc)))\n",
    "    plt.title(view_name+\"\\n\",fontsize=17)\n",
    "    plt.xlim([0,1])\n",
    "    plt.ylim([0,1])\n",
    "    for i in range(n):\n",
    "        plt.plot(fpr[i][1],tpr[i][1],color=\"b\",marker='o',alpha=0.3)\n",
    "    plt.savefig(save_name+'tkt_roc_curve.png',bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check path\n",
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "for rate_cnt in range(1):\n",
    "    if rate_cnt == 1:\n",
    "        rate = 1e-3\n",
    "        \n",
    "    elif rate_cnt == 0:\n",
    "        rate = 1e-4\n",
    "        \n",
    "    elif rate_cnt == 2:\n",
    "        rate = 1e-5\n",
    "        \n",
    "    elif rate_cnt == 3:\n",
    "        rate = 1e-6\n",
    "        \n",
    "    for batch_cnt in range(5):\n",
    "        if batch_cnt == 0:\n",
    "            batch = 16\n",
    "            \n",
    "        elif batch_cnt == 1:\n",
    "            batch = 32\n",
    "            \n",
    "        elif batch_cnt == 2:\n",
    "            batch = 64\n",
    "            \n",
    "        elif batch_cnt == 3:\n",
    "            batch = 128\n",
    "            \n",
    "        elif batch_cnt == 4:\n",
    "            batch = 256\n",
    "        \n",
    "        kf = KFold(n_splits=5)\n",
    "        cnt=1\n",
    "        for train_idx, val_idx in kf.split(trainvalid_0_img2):\n",
    "            train_img = np.concatenate([trainvalid_0_img2[train_idx], trainvalid_1_img2[train_idx]])\n",
    "            train_label_ndarray = np.concatenate([trainvalid_label_0_ndarray[train_idx], trainvalid_label_1_ndarray[train_idx]])\n",
    "            valid_img = np.concatenate([trainvalid_0_img2[val_idx], trainvalid_1_img2[val_idx]])\n",
    "            valid_label_ndarray = np.concatenate([trainvalid_label_0_ndarray[val_idx], trainvalid_label_1_ndarray[val_idx]])\n",
    "            train_label = []\n",
    "            valid_label = []\n",
    "            for i in train_label_ndarray:\n",
    "                train_label.append(i)\n",
    "            for t in valid_label_ndarray:\n",
    "                valid_label.append(t)\n",
    "            train_amount=len(train_label)\n",
    "            from keras import optimizers\n",
    "            if opt==\"Adam\":\n",
    "                optimizer=optimizers.Adam(lr=rate)\n",
    "            elif opt=='AMSGrad':\n",
    "                optimizer=optimizers.Adam(lr=rate,amsgrad=True)\n",
    "            elif opt==\"RMS\":\n",
    "                optimizer=optimizers.RMSprop(lr=rate)\n",
    "            elif opt=='SGD':\n",
    "                optimizer=optimizers.SGD(lr=rate)\n",
    "            elif opt=='adamax':\n",
    "                optimizer=optimizers.Adamax(lr=rate)\n",
    "                \n",
    "            save_name,save_view_name=folder_path(result_folder,date_folder,title,models,case,opt,dataset,train_amount,cnt,rate,batch,epochs)\n",
    "            print(\"-----------------------------------------------\")\n",
    "            print(save_name)\n",
    "            print(\"png_viwe:  \"+save_view_name)\n",
    "            print(\"- - - - - - - - - - - - - - - - - - - - - - - -\")\n",
    "            if (os.path.exists(save_name)==True):\n",
    "                print(\"exist// skip\")\n",
    "            cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_start=time.time()\n",
    "size=224\n",
    "for rate_cnt in range(4):\n",
    "    if rate_cnt == 0:\n",
    "        rate = 1e-7\n",
    "    \n",
    "    elif rate_cnt == 1:\n",
    "        rate = 1e-4\n",
    "        \n",
    "    elif rate_cnt == 2:\n",
    "        rate = 1e-5\n",
    "        \n",
    "    elif rate_cnt == 3:\n",
    "        rate = 1e-6\n",
    "        \n",
    "    for batch_cnt in range(4):\n",
    "        if batch_cnt == 0:\n",
    "            batch = 16\n",
    "            \n",
    "        elif batch_cnt == 1:\n",
    "            batch = 32\n",
    "            \n",
    "        elif batch_cnt == 2:\n",
    "            batch = 64\n",
    "            \n",
    "        elif batch_cnt == 3:\n",
    "            batch = 128\n",
    "            \n",
    "        # elif batch_cnt == 4:\n",
    "        #     batch = 256\n",
    "        \n",
    "        kf = KFold(n_splits=5)\n",
    "        cnt=1\n",
    "        for train_idx, val_idx in kf.split(trainvalid_0_img2):\n",
    "            train_img = np.concatenate([trainvalid_0_img2[train_idx], trainvalid_1_img2[train_idx]])\n",
    "            train_label_ndarray = np.concatenate([trainvalid_label_0_ndarray[train_idx], trainvalid_label_1_ndarray[train_idx]])\n",
    "            valid_img = np.concatenate([trainvalid_0_img2[val_idx], trainvalid_1_img2[val_idx]])\n",
    "            valid_label_ndarray = np.concatenate([trainvalid_label_0_ndarray[val_idx], trainvalid_label_1_ndarray[val_idx]])\n",
    "            train_label = []\n",
    "            valid_label = []\n",
    "            for i in train_label_ndarray:\n",
    "                train_label.append(i)\n",
    "            for t in valid_label_ndarray:\n",
    "                valid_label.append(t)\n",
    "            print(\"cnt\",str(cnt),valid_label)\n",
    "            #ワンホット変換 #学習用（hist）\n",
    "            train_label_list = np_utils.to_categorical(train_label,num_class)\n",
    "            valid_label_list = np_utils.to_categorical(valid_label,num_class)\n",
    "            print(\"cnt\",str(cnt),valid_label_list)\n",
    "            \n",
    "            train_amount=2425\n",
    "            from keras import optimizers\n",
    "            if opt==\"Adam\":\n",
    "                optimizer=optimizers.Adam(lr=rate)\n",
    "            elif opt=='AMSGrad':\n",
    "                optimizer=optimizers.Adam(lr=rate,amsgrad=True)\n",
    "            elif opt==\"RMS\":\n",
    "                optimizer=optimizers.RMSprop(lr=rate)\n",
    "            elif opt=='SGD':\n",
    "                optimizer=optimizers.SGD(lr=rate)\n",
    "            elif opt=='adamax':\n",
    "                optimizer=optimizers.Adamax(lr=rate)\n",
    "                \n",
    "            save_name,save_view_name=folder_path(result_folder,date_folder,title,models,case,opt,dataset,train_amount,cnt,rate,batch,epochs)\n",
    "            print(\"-----------------------------------------------\")\n",
    "            print(save_name)\n",
    "            print(\"png_viwe:  \"+save_view_name)\n",
    "            print(\"- - - - - - - - - - - - - - - - - - - - - - - -\")\n",
    "            \n",
    "            os.makedirs(save_name,exist_ok=True)\n",
    "            print(\"make folder & learn start \")\n",
    "    \n",
    "            model=make_model(size,num_class,optimizer,case,mode,read_name)\n",
    "        \n",
    "            # コールバック関連設定\n",
    "            # モデル保存用のディレクトリ\n",
    "            model_dir = os.path.join(save_name,'models')\n",
    "            os.makedirs(model_dir,exist_ok = True)\n",
    "            weights_dir = os.path.join(model_dir,'weights')\n",
    "            os.makedirs(weights_dir,exist_ok = True)\n",
    "           # 重みを1エポックごとに保存\n",
    "            cp_filepath =  os.path.join(weights_dir, 'ep_{epoch:02d}_vls_{val_loss:.1f}.h5')\n",
    "            cp = ModelCheckpoint(cp_filepath, \n",
    "                                 monitor='val_loss', \n",
    "                                 verbose=0,\n",
    "                                 save_best_only=False, \n",
    "                                 save_weights_only=True, \n",
    "                                 mode='auto', \n",
    "                                 period=5\n",
    "                                 )\n",
    "            # EaelyStoppingの設定\n",
    "            early_stopping = EarlyStopping(monitor='loss',\n",
    "                                           min_delta=0.2,\n",
    "                                           patience=30\n",
    "                                           )\n",
    "            \n",
    "            # 学習の実行\n",
    "            hist=model.fit(train_img,\n",
    "                           train_label_list,\n",
    "                           batch_size=batch,\n",
    "                           epochs=epochs,\n",
    "                           validation_data=(valid_img,valid_label_list),\n",
    "                           callbacks=[cp,early_stopping]\n",
    "                           )\n",
    "            # いろいろ保存\n",
    "            save_model_data(model_dir,model,hist,test_img)\n",
    "            # 学習曲線\n",
    "            fig_size_Width=12\n",
    "            fig_size_Height=10\n",
    "            fig_Font_size=25\n",
    "            plot_history(hist,\n",
    "                         save_path=save_name,\n",
    "                         fig_size_width=fig_size_Width,\n",
    "                         fig_size_height=fig_size_Height,\n",
    "                         lim_font_size=fig_Font_size)\n",
    "            cm_file_name=save_name+\"CM_train.png\"\n",
    "            color=\"Reds\"\n",
    "            pred_list=Confusion_matrix(model,train_img,train_label,cm_file_name,color,save_view_name)\n",
    "    \n",
    "            cm_file_name=save_name+\"CM_valid.png\"\n",
    "            color=\"Blues\"\n",
    "            pred_list=Confusion_matrix(model,valid_img,valid_label,cm_file_name,color,save_view_name)\n",
    "\n",
    "            color='Greens'\n",
    "            cm_file_name=save_name+\"CM_test.png\"\n",
    "            pred_list=Confusion_matrix(model,test_img,test_label,cm_file_name,color,save_view_name)\n",
    "    \n",
    "            #  閾値変化のroc曲線\n",
    "            ROC_CURVE(model,test_img,test_label,save_name,save_view_name)\n",
    "            cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_time=time.time()-learn_start\n",
    "learn_time/=60\n",
    "print(\"{:.04}min in learn\".format(learn_time))\n",
    "print(\"finish\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
